[{"content": ["<div class=\"entry-content\">\n<p>I am an Assistant Professor of Writing and Rhetoric at the University of Rhode Island. I teach courses in rhetoric, professional writing, and multi-modal composition. I am also an affiliated researcher with Michigan State University\u2019s <a href=\"www2.matrix.msu.edu/wide/\">WIDE@MATRIX.</a></p>\n<p>My research interests includes computational approaches to rhetorical analysis. I am currently leading a project that is developing an app that will automatically track and analyze facilitation strategies in online learning environments.</p>\n<p>I received my PhD. in Rhetoric, Composition, and Digital Media from The Ohio State University in 2012.</p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>"], "start_url": "http://ryan-omizo.com/", "links": ["http://ryan-omizo.com/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments-blog-page/", "#content", "https://twitter.com/omizorm", "www2.matrix.msu.edu/wide/", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F&via=OmizoRm", "https://twitter.com/intent/follow?screen_name=OmizoRM", "https://twitter.com/OmizoRM", "http://ryan-omizo.com/experiments-blog-page/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments/", "http://ryan-omizo.com/2016/04/07/2016-cccc-presention-eli-review-for-research/", "http://ryan-omizo.com/2016/01/11/finding-genre-signals-in-academic-writing-benchmarking-method/", "http://ryan-omizo.com/2015/05/27/hastac2015-lightning-talk-visualizing-computational-rhetoric/", "http://ryan-omizo.com/2015/01/16/python-recipe-for-nsf-proposals/", "http://ryan-omizo.com/2014/10/27/text-mining-reuters-of-course/", "http://ryan-omizo.com/wp-login.php", "http://ryan-omizo.com/feed/", "http://ryan-omizo.com/comments/feed/", "https://wordpress.org/"], "title": ["ryan-omizo.com | "]},
{"content": ["<div class=\"entry-content\">\n<div id=\"attachment_40\" style=\"width: 310px\" class=\"wp-caption alignnone\"><a href=\"http://i1.wp.com/ryan-omizo.com/wp-content/uploads/2013/06/poster_graph-e1371333261488.png\"><img src=\"http://i0.wp.com/ryan-omizo.com/wp-content/uploads/2013/06/poster_graph-e1371333261488-300x225.png?resize=300%2C225\" alt=\"Network Graph of Discussion Board Posts\" class=\"size-medium wp-image-40\" srcset=\"http://i1.wp.com/ryan-omizo.com/wp-content/uploads/2013/06/poster_graph-e1371333261488.png?resize=300%2C225 300w, http://i1.wp.com/ryan-omizo.com/wp-content/uploads/2013/06/poster_graph-e1371333261488.png?w=1024 1024w\" sizes=\"(max-width: 300px) 100vw, 300px\" data-recalc-dims=\"1\"></a><p class=\"wp-caption-text\">Network Graph of Discussion Board Posts</p></div>\n<p>My research has involved examining rhetorical configurations of the human face in vernacular video texts that populate networks such as YouTube. I have focused on explaining how the assumptions about what constitutes a typical human face inflects discourses of celebrity, race, and international human rights campaigns.</p>\n<p>As a post-doctoral research for MSU\u2019s WIDE-MATRIX, I investigated how computational methodologies from linguistics, graph theory, and statistical natural language processing can inform the theories and methods of a <em>computational rhetoric</em>.</p>\n<p>In my current capacity as an Assistant Professor of Writing and Rhetoric at the University of Rhode Island, I am involved in long term research study of how computational approaches can be used to improve student peer review. Additionally, I build web apps that use machine learning to conduct automatic rhetorical analysis. One such web app project is the Faciloscope.</p>\n<p>The Faciloscope grew out of a IMLS grant-funded study of informal learning practices supervised by an assortment of museums, including the Science Museum and Minnesota.</p>\n<p><script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');</p>\n<p>  ga('create', 'UA-41733568-1', 'ryan-omizo.com');\n  ga('send', 'pageview');</p>\n<p></script></p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2Fresearch-page%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>"], "start_url": "http://ryan-omizo.com/research-page/", "links": ["http://ryan-omizo.com/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments-blog-page/", "#content", "https://twitter.com/omizorm", "http://i1.wp.com/ryan-omizo.com/wp-content/uploads/2013/06/poster_graph-e1371333261488.png", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2Fresearch-page%2F&via=OmizoRm", "https://twitter.com/intent/follow?screen_name=OmizoRM", "https://twitter.com/OmizoRM", "http://ryan-omizo.com/experiments-blog-page/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments/", "http://ryan-omizo.com/2016/04/07/2016-cccc-presention-eli-review-for-research/", "http://ryan-omizo.com/2016/01/11/finding-genre-signals-in-academic-writing-benchmarking-method/", "http://ryan-omizo.com/2015/05/27/hastac2015-lightning-talk-visualizing-computational-rhetoric/", "http://ryan-omizo.com/2015/01/16/python-recipe-for-nsf-proposals/", "http://ryan-omizo.com/2014/10/27/text-mining-reuters-of-course/", "http://ryan-omizo.com/wp-login.php", "http://ryan-omizo.com/feed/", "http://ryan-omizo.com/comments/feed/", "https://wordpress.org/"], "title": ["Research"]},
{"content": ["<div class=\"entry-content\">\n<div id=\"teaching\">\n<h3>As a writing teacher, I believe that there are 3 core principles that inform how we learn and grow as writers:</h3>\n<p><b>Write.</b> To learn to write well, we must write often. What some might call rigor, I would call exercise and stamina, both of which demand an ability to focus and slog through obstacles.</p>\n<p><b>Review.</b> To learn to write well, we must also be able to recognize, not merely grammatical correctness or domain knowledge, but the cues of writing that locate a text in a particular genre and direct it to a particular audience. This requires an ability to review the writing of others and yourself.\n</p><p><b>Revise.</b> No matter what level, all writers must revise. They must marshal the feedback given in peer or self-reviews and incorporate that feedback in the re-drafting of the text. Writers must determine what changes are needed, and, importantly, what should remain.</p>\n<h2>Recent Courses</h2>\n<dl>\n<dt>WRT 645: Graduate Seminar in Rhetoric</dt>\n<dd>Rhetoric and the Digital Humanities</dd>\n<dt>WRT 495: Senior Capstone Course</dt>\n<dd></dd>\n<dt>WRT 235: Composing in Electronic Environments</dt>\n<dd></dd>\n<dt>WRT 227: Business Communication</dt>\n<dd></dd>\n</dl>\n</div>\n<p><!-- close #teaching --></p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2Fteaching-page%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>"], "start_url": "http://ryan-omizo.com/teaching-page/", "links": ["http://ryan-omizo.com/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments-blog-page/", "#content", "https://twitter.com/omizorm", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2Fteaching-page%2F&via=OmizoRm", "https://twitter.com/intent/follow?screen_name=OmizoRM", "https://twitter.com/OmizoRM", "http://ryan-omizo.com/experiments-blog-page/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments/", "http://ryan-omizo.com/2016/04/07/2016-cccc-presention-eli-review-for-research/", "http://ryan-omizo.com/2016/01/11/finding-genre-signals-in-academic-writing-benchmarking-method/", "http://ryan-omizo.com/2015/05/27/hastac2015-lightning-talk-visualizing-computational-rhetoric/", "http://ryan-omizo.com/2015/01/16/python-recipe-for-nsf-proposals/", "http://ryan-omizo.com/2014/10/27/text-mining-reuters-of-course/", "http://ryan-omizo.com/wp-login.php", "http://ryan-omizo.com/feed/", "http://ryan-omizo.com/comments/feed/", "https://wordpress.org/"], "title": ["Teaching"]},
{"content": ["<div class=\"entry-content\">\n<div id=\"cv\">\n<dl>\n<dt>\n<h1>Professional Experience</h1>\n</dt>\n<dd>Assistant Professor of Writing and Rhetoric, University of Rhode Island, Present</dd>\n<dd>Visiting Assistant Professor of Professional and First-year Writing, Michigan State University, 2012-2014</dd>\n<dd>MSU WIDE-MATRIX Post-doctoral Researcher in Computational Rhetorics, 2012-2014</dd>\n<dt>\n<h1>Education</h1>\n<dt>\n</dt></dt><dd>Ph.D. Rhetoric and Composition, The Ohio State University, 2012</dd>\n<dd>MA English, University of Hawai\u2019i at Manoa, 2007</dd>\n<dd>BA English, University of Hawai\u2019l at Manoa, 2002</dd>\n<dt>\n<h1>Publications</h1>\n</dt>\n<dd>\u201c<a href=\"http://enculturation.gmu.edu/vulnerable-video\">Omizo, Ryan. \u201cVulnerable Video: A New Vernacular.\u201d Enculturation: A Journal of Writing, Rhetoric, and Culture. 8 (October 2010). Web.</a></dd>\n<dd>\u201d<a href=\"http://www.worcester.edu/Currents/Archives/Volume_1_Number_1.aspx\">Henry, Jim, H. Bruland, and Ryan Omizo. \u201cMentoring first-year students in composition: Tapping role construction to teach.\u201d Currents in Teaching and Learning 1.1 (2008): 17-28.</a></dd>\n<dd><a href=\"http://kairos.technorhetoric.net/13.1\">Omizo, Ryan. \u201cDigital Media and Composition: DMAC Theory 2008.\u201d Kairos: a Journal of Rhetoric, Technology, and Pedagogy. 13.1 (2008). Web.</a></dd>\n<dd>\n<h3><em>Publications Accepted</em></h3>\n</dd>\n<dd>\u201cCultivating the Global Subject of Human Rights Pedagogy\u201d (with Wendy Hesford) Human Rights Literature and Pedagogy (Endorsed by the Executive Publications Committee at MLA). Eds. Elizabeth Goldberg and Alexandra Stielmeyer. Chapter submitted March 2009.</dd>\n<dd>\u201cParticipation and the Problem of Measurement.\u201d Rhetoric of Participation: Interrogating the Commonplace In and Beyond the Classroom. Collection approved by Computers and Composition Digital Press, an imprint of Utah State University Press. Eds. Paige V. Banaji, Lisa Blankenship, Katie DeLuca, Lauren Obermark.</dd>\n<dt>\n<h1>Presentations</h1>\n</dt>\n<dd>\n<h3><em>Invited Presentations</em></h3></dd>\n<dd>\u201cComputational Rhetorics: Addressing the Big Questions of Digital Humanities.\u201d Midwestern Conference of Asian Affairs. East Lansing, MI. October 2013.</dd>\n<dd>\u201cDesigning Global Persuasion: Using Graph Theory to Mine Social Network Data.\u201d SIGDOC 2013. Greenville, NC. October 2013.</dd>\n<dd>\u201cTransnational Composition.\u201d  Conference of College Composition and Communication Convention. Special Interest Group Presentation, Atlanta, Georgia. April 2011. (with Wendy S. Hesford)</dd>\n<dd>\n<h3><em>Paper Presentations</em></h3>\n</dd>\n<dd>\u201cMapping Technical Communication: Using SVM Classification to Identify Rhetorical Moves in the Enron Email Corpus.\u201d ATTW 2014. Indianapolis, Indiana. May 2014.</dd>\n<dd>\u201cComputational Rhetorics: Adapting Graph-theory Analytics to Big Data.\u201d DH2013. Lincoln, Nebraska. July 2013</dd>\n<dd>\u201cComputational Rhetoric: An Invitation to the Laboratory.\u201d Computers and Writing. Frostburg, Maryland. June 2013.</dd>\n<dd>\u201cAdventures in Graphing.\u201d WIDE-Emu Conference. East Lansing, Michigan. October 2012.</dd>\n<dd>\u201cFacing Online Video: Toward a Rhetorical Physiognomy.\u201d  Conference of College Composition and Communication.  St. Louis, Missouri. March 2012.</dd>\n<dd>\u201cCritical Technological Performance: Teaching From the Fronts to the Backs.\u201d Conference on College Composition and Communication Convention, Atlanta, Georgia. April 2011.</dd>\n<dd>\u201cRemixing Human Rights: Towards an Intercontexual Rhetoric of Human Rights Media.\u201d Conference of College Composition and Communication Convention, Louisville, Kentucky. March 2010.</dd>\n<dd>\u201cDMAC Theory: 2008.\u201d  Watson Conference, Louisville, Kentucky. 2008. Computer installation exhibit.\n</dd>\n<dd>\u201cPosthumanism versus an Archaeology of the Mundane\u201d Conference of College Composition and Communication Convention, New York, New York. March 2007.</dd>\n</dl>\n</div>\n<p><!-- close #cv --></p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2Fcv-page%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>"], "start_url": "http://ryan-omizo.com/cv-page/", "links": ["http://ryan-omizo.com/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments-blog-page/", "#content", "https://twitter.com/omizorm", "http://enculturation.gmu.edu/vulnerable-video", "http://www.worcester.edu/Currents/Archives/Volume_1_Number_1.aspx", "http://kairos.technorhetoric.net/13.1", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2Fcv-page%2F&via=OmizoRm", "https://twitter.com/intent/follow?screen_name=OmizoRM", "https://twitter.com/OmizoRM", "http://ryan-omizo.com/experiments-blog-page/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments/", "http://ryan-omizo.com/2016/04/07/2016-cccc-presention-eli-review-for-research/", "http://ryan-omizo.com/2016/01/11/finding-genre-signals-in-academic-writing-benchmarking-method/", "http://ryan-omizo.com/2015/05/27/hastac2015-lightning-talk-visualizing-computational-rhetoric/", "http://ryan-omizo.com/2015/01/16/python-recipe-for-nsf-proposals/", "http://ryan-omizo.com/2014/10/27/text-mining-reuters-of-course/", "http://ryan-omizo.com/wp-login.php", "http://ryan-omizo.com/feed/", "http://ryan-omizo.com/comments/feed/", "https://wordpress.org/"], "title": ["CV"]},
{"content": ["<div class=\"entry-content\">\n<p><strong>The following post functions as supplementary material for \u201cFinding Genre Signals in Academic Writing\u201d for the <em>Journal of Writing Research. </em>This post explains how we automatically processed 505 research articles from the Spring OpenAccess database to filter citational sentences from non-citational sentences. While the primary analysis of \u201cFinding Genre Signals in Academic Writing\u201d relies on hand-coded sentences, the authors have developed this automated routine in order to test the viability of our citational coding scheme (which targets the lexical content of the sentence) and with an eye toward future citation analysis projects that may benefit from automated analysis.<br>\n</strong></p>\n<p>To gather citation data in order to \u00a0benchmark our coding scheme, surface-level parser, and gain a global sense of how Extraction, Grouping, and Author(s) as Actant(s) citational types operated within the larger field of academic research, we screen scraped 505 research articles from journals hosted by Springer OpenAccess. These journals are peer reviewed and write to the genre conventions of academic audiences, including the Introduction-Methods-Results-Discussion (IMRaD) format often used to structure scientific and social scientific journals (see Christensen and Kawakami, 2009; Hannick and Flanigan, 2013; Salager-Meyer, 1994; \u00a0. This screen scrape captured the meta-data of the article (author names, date of publication, institutional affiliation, and document object index), the full text of the article without images, and works cited list. Only articles types labeled as \u201cResearch Article\u201d by the Springer OpenAccess filtering tool were used for this exploratory analysis.</p>\n<p>We then tokenize each article at the sentence level. Using regular expression searches, we tagged all in-text citations and non-citations. For this study, an in-text citation denotes a sentence-token that attributes a source via author name or author name and date of publication in Harvard-style in-text citation formatting. Because the citation style varied across journals due to vagaries in HTML markup presentations, we narrowed our selection to those journals that employed the following in-text citation patterns:</p>\n<p>Author last name (Year of Publication)<br>\n2 author last names (Year of Publication)<br>\nFirst author last name, et al. (Year of Publication)<br>\nAuthor last name (Year of Publication + a-z index where different articles by the same authors appear)<br>\n2 author last names (Year of Publication + a-z index where different articles by the same authors appear)<br>\nFirst author last name, et al. (Year of Publication + a-z index where different articles by the same authors appear)<br>\nAuthor last name [Year of Publication]<br>\n2 author last names [Year of Publication]<br>\nFirst author last name, et al. [Year of Publication]<br>\nAuthor last name [Year of Publication + a-z index where different articles by the same authors appear]<br>\n2 author last names [Year of Publication + a-z index where different articles by the same authors appear]<br>\nFirst author last name, et al. [Year of Publication + + a-z index where different articles by the same authors appear]<br>\nAuthor last name ([Year of Publication])<br>\n2 author last names ([Year of Publication])<br>\nFirst author last name, et al. ([Year of Publication])<br>\nAuthor last name ([Year of Publication + a-z index where different articles by the same authors appear])<br>\n2 author last names ([Year of Publication + a-z index where different articles by the same authors appear])<br>\nFirst author last name, et al. ([Year of Publication + + a-z index where different articles by the same authors appear])<br>\nAuthor last name ([Year of Publication])<br>\n2 author last names ([Year of Publication])<br>\nFirst author last name, et al. ([Year of Publication])<br>\n(Author last name Year of Publication)<br>\n(2 author last names [Year of Publication])<br>\n(First author last name, et al. [Year of Publication])</p>\n<p>Works cited entries were excluded from this pattern matching. In addition, statements that may be considered citational in nature, but did not contain explicit references to authors or dates of publication were also excluded. For example, the sentence from Ogada, et al. (2014):</p>\n<p>These authors concluded that initial adoption may be low due to imperfect information on management and profitability of the new technology but as this becomes clearer from the experiences of their neighbors and their own experience, adoption is scaled up.\u201d</p>\n<p>While this sentence functions to synthesize the work of several authors previously cited in the article by Ogada, et al. (2014), it does not contain markers of author attribution or date of publication. Thus, interpretive sentences of this type were not included in the initial in-text citation search. Although we do see the potential contribution of tracking these rhetorical moves of extended synthesis, making judgments about the nature of such moves proved difficult for the lexical pattern matching routines.</p>\n<p>For sentence that name authors, but do not provide a date of publication, we configured the screen scraper program to parse the DOM tree of the article for its References section. The last name of the primary author of the publication is sequestered into a list. If an in-text citation has stumped the initial regular expression searching parameters and received a tag of \u201cnon-intext-citation\u201d, then the script will check for the presence of the primary author\u2019s last name in the sentence by comparing the extant words with the list of author last names compiled in the screen scrape. To preclude radical changes, only the names greater than two characters in length are retained. If there is a match between the first author\u2019s last name and a word in the \u201cnon-intext-citation\u201d sentence, the tag is changed to \u201cintext-citation.\u201d This update of the search protocol assumes that the correspondence between a capitalized word and an author name listed in the reference section of the article most likely indicates an in-text citation. In some cases, the author\u2019s last name could also function as a content word (verb or noun), leading to a falsely assigned label. For a generic example, consider an author whose last name is \u201cHouse.\u201d The entry \u201cHouse\u201d would not match \u201chouse\u201d because the latter lacks an initial capital letter; however, a sentence containing the collocation \u201cWhite House\u201d would lead to a false positive. Another false permutation that we have encountered in the study occurs when an article is discussing an organization and cites work produced by that organization or other organizations that share a similar appellation. For example, in Rissler, et al. (2014), the authors write:</p>\n<p>In the only nationwide survey of high school science teachers (n?=?939), Berkman et al ([2008]) found that at least 17% of biology teachers are young-earth creationists, and about one in eight teach creationism or intelligent design in a positive light.</p>\n<p>Only 23% of teachers strongly agreed that evolution is the unifying theme of biology, as accepted by the National Academy of Science and the National Research Council.</p>\n<p>The first sentence from Rissler et al (2014) is tagged as a citation because of the reference to \u201cBerkman et al ([2008]).\u201d The information of the second sentence has been sourced from the previous sentence. By our thin definition of what constitutes and in-text citation, the second sentence should not be tagged; however, because the word \u201cCouncil\u201d is present in the second sentence and \u201cCouncil\u201d is included in the article\u2019s reference list in the position of a last name, the second sentence is classed as an in-text citation. We consider this cross-referencing step a contingency for articles that may present copy-editing inconsistent with the journal style guide. However, the bank of names harvested from the reference sections of article is reused in a subsequent processing step, which replaces an instance of an author\u2019s name in the text with the cognate tag \u201cAUTHOR.\u201d</p>\n<p>When one of the above citational conditions are met by the lexical content of a sentence, that sentence is tagged as an in-text citation (1). Those sentences that do not match the lexical patterns above receive a non-in-text citation (0) tag.</p>\n<p>After initial processing by the screen scraping and the in-text citation/non-in-text citation tagging routines, we then pass the marked sentences\u2013now annotated with a 0 or 1\u2013to a second processing module, whose goal is to reduce the complexity of the syntactic complexity of the in-text citation to more general cognates. This second processing module makes the following substitutions:<br>\npublication years featured in in-text citations are replaced with the tag \u201cPUBYEAR\u201d<br>\nan author\u2019s last name, if found in the list of names harvested from the reference section of the article, is replaced with the tag \u201cAUTHOR\u201d<br>\npart of speech are tagged by a pre-trained pos-tagger, which relies on the Penn Part of Speech tags (see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html); only those parts of speech tags which indicate verbs, prepositions, and determines are retained and inserted into the body of the sentence</p>\n<p>As an example, we can consider the following sentence from Otten, et al. (2015):</p>\n<p>Product Portfolio Management (PPM) is a dynamic decision process, whereby a business list of active (new) products (and R&amp;D) projects is constantly updated and revised (Cooper, Edgett, &amp; Kleinschmidt, [2001]).</p>\n<p>Given the above processing step, that sentence would be transformed into:</p>\n<p>Product Portfolio Management (PPM) VBZ is DT dynamic decision process, whereby DT businesss list IN active (new) products (and R&amp;D) projects VBZ is constantly updated and VBN revised (AUTHOR, Edgett, &amp; Kleinschmidt, PUBYEAR).\u2019</p>\n<p>After each sentence is tagged by selected parts of speech, AUTHOR, and PUBYEAR, the configuration and/or quantity of the tags are assessed in a third processing module. This third processing module applies the citational coding scheme discussed above as numerical tags: Extraction (1), Grouping (2), and Author(s) as Actant(s) (3) by comparing the parts of speech, AUTHOR, and PUYEAR tags to hard-coded lexical patterns fitted to each category of the coding scheme. If a sentence contains parts of speech, AUTHOR, and PUBYEAR tags that match the category of Extraction in-text citation, then taht sentence will receive a 1, and so on. This processing module works through elimination:</p>\n<p>ignore all sentences tagged as non-in-text citations<br>\ntag all in-text citations in which PUBYEAR appears more than 2x as Grouping (2)<br>\ncompare remaining sentences (i.e., not Grouping) with AUTHOR, parts of speech, and PUBYEAR pattern and designate matches as Author(s) as Actant(s) (3)<br>\ntag all remaining in-citations (i.e., not Group or Author(s) as Actant(s)) as Extraction (1)</p>\n<p>In one sense, the third processing module moves from the most deterministic coding category to the least deterministic. For our coding scheme, any in-text citation that refers to more 3 or more sources within the boundary of the sentence is considered Grouping, regardless of the grammatical construction or the presence of other features that may match Author(s) as Actant(s) or Extraction. We may consider an example sentence from Otten, et al. (2015):</p>\n<p>A viable alternative for determining the product portfolio (product assortment) is the use of a data mining approach, called Association Rule Mining (ARM), which exposes the interrelationships between products by inspecting a transactional dataset (Brijs, Swinnen, Vanhoof, &amp; Wets, [1999]; [2004]; Fayyad, Piatetsky-Shapiro, &amp; Smyth [1996a], [1996b], [1996c].</p>\n<p>An annotated example of the above Grouping sentence would appear like the following:</p>\n<p>DT viable alternative IN determining DT product portfolio (product assortment) VBZ is DT use IN DT data mining approach, VBN called Association Rule Mining (ARM), which exposes DT interrelationships IN products IN inspecting DT transactional dataset (AUTHOR, Swinnen, Vanhoof, &amp; Wets, PUBYEAR; PUBYEAR; Fayyad, Piatetsky-Shapiro, &amp; PUBYEAR, PUBYEAR, PUBYEAR.\u2019 (Otten et al. 2015)</p>\n<p>The next most deterministic category is Author(s) as Actant(s) because this category demands that an author be named in the sentence and function as the subject or as the receive of an action and that references to other sources be less than 3. Because the Author(s) as Actant(s) category cannot contain more than 2 references, it is excluded from the Grouping category by default. It is excluded from the Extraction category because it will contain a direct authorial attribution in which the named author is performing the action of the sentence or the object of the verb of the sentence. Take for example the following sentence from Correa Bahnsen, et al. (2015):</p>\n<p>Moreover, as discussed in (Verbraken et al [2013]), if the average instead of the total profit is considered and the fixed cost A is discarded since is irrelevant for classifier selection, the profit can be expressed as: (2) Nevertheless, equations (1) and (2), assume that every customer has the same CLV and Co, whereas this is not true in practice.</p>\n<p>The above sentence would be tagged in the following manner:</p>\n<p>Moreover, IN VBN discussed IN (AUTHOR et al PUBYEAR), IN DT average instead IN DT total profit VBZ is VBN considered and DT VBN fixed cost DT VBZ is VBN discarded IN VBZ is irrelevant IN classifier selection, DT profit can VB be VBD expressed as: (2) Nevertheless, equations (1) and (2), VBP assume IN DT customer VBZ has DT same CLV and Co, whereas DT VBZ is not true IN practice.\u2019</p>\n<p>In the above example, the key sequence is \u201cIN VBN discussed IN (AUTHOR et al PUBYEAR).\u201d The pattern of past participle verb tag + past participle verb + preposition + AUTHOR tag + PUBYEAR tag corresponds to a pre-existing arrangement in the module 3 processor, which assumes that an AUTHOR tag immediately following a verb clause indicates that an action is being attributed to a named author.</p>\n<p>All in-text citation sentences have not received a Grouping (2) or Author(s) as Actant(s) (3) classifications are then automatically tagged as Extraction (1). Programmatically, an Extraction (1) classification is any in-text citation that has less than three PUBYEAR tags and does not attribute action to a named author within the boundaries of the sentence by making the author the subject or object of an action in an independent or subordinate clause. An example from Correa Bahnsen, et a. (2015) would be:</p>\n<p>This assumption does not hold in many real-world applications such as churn modeling, since when misidentifying a churner the financial losses are quite different than when misclassifying a non-churner as churner (Glady et al [2009]).</p>\n<p>After processing, the above sentence would be tagged as:</p>\n<p>DT assumption VBZ does not VBP hold IN many real-world applications such IN churn modeling, IN when misidentifying DT churner DT financial losses VBP are quite different IN when misclassifying DT non-churner IN churner (AUTHOR et al PUBYEAR).\u2019</p>\n<p>As we noted in the beginning of the article, the ultimate aim of our work is to accomplish two tasks: (1) compare advisor and advisee texts and (2) output measures of comparison that inform a rhetorical reading of citational moves in academic writing. Doing so means converting raw advisor and advisee texts into a computational objects and selecting features from those objects that offer relevant quantitative and qualitative information. In this first pass, the computational object was a \u201cstring.\u201d In the next pass, we convert texts to another kind of computational object, a graph, for further analysis.</p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2016%2F01%2F11%2Ffinding-genre-signals-in-academic-writing-benchmarking-method%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<p>In a recent <a href=\"http://politico.com\">Politico</a> article, <a href=\"http://www.politico.com/magazine/story/2015/01/no-the-gop-is-not-at-war-with-science-114195.html#.VLiTY3YppbQ\">\u201cNo, the GOP is Not at War with Science\u201d</a>, Senator Rand Paul and Representative Lamar Smith addressed criticism of the GOP\u2019s attempt to undermine the peer review process of the National Science Foundation and National Institute of Health\u2019s grant proposal system.</p>\n<p>In their <a href=\"http://www.politico.com/magazine/story/2015/01/no-the-gop-is-not-at-war-with-science-114195.html#.VLiTY3YppbQ\">article</a>, Paul and Smith cite what they suggest are frivolous expenditures on research that do not advance national interests. I will not name the studies listed by Paul and Smith, but I do encourage people to read the original article to understand the jeers in context.</p>\n<p>I will, however, cite one of Paul and Smith\u2019s justifications for their argument:</p>\n<blockquote><p>Our national debt is more than $18 trillion, and the American taxpayer is hurting. If we, as a country, have decided to spend taxpayers\u2019 hard-earned dollars on funding science and research, then we need to spend wisely. Every dollar spent by the federal government must be spent just as the typical family deals with spending decisions on car payments, child care, food purchases and housing needs.</p>\n<p>Read more: http://www.politico.com/magazine/story/2015/01/no-the-gop-is-not-at-war-with-science-114195.html#ixzz3OxJqbYXx</p></blockquote>\n<p>In order to help researchers appeal to the financial and national scruples of the Republican controlled Congress, I have written a short script that will automatically generate a GOP-friendly NSF grant proposal description in Word.</p>\n<p>The Python code (below) requires only the <code>python-docx</code> and <code>lxml &gt;= 2.32</code> packages that can be downloaded through <code>pip</code>.</p>\n<pre class=\"height:1000 lang:default decode:true \" title=\"Python Script for GOP-Friendly NSF Proposals\">__author__ = 'Ryan Omizo'\r\n\r\n#Requires python-docx package\r\n#Requires lxml &gt;=2.3.2\r\n\r\nfrom docx import Document\r\nimport random\r\n\r\ndef guns_references():\r\n    dates = [2001, 2007, 2012, 1998, 2002, 2003, 2010, 1985, 1997]\r\n    references = ['Guns, Gun. Gun. (' + str(dates[i]) + '). Guns. Guns: Guns.' for i in range(len(dates))]\r\n\r\n    return references\r\n\r\ndef guns_intext():\r\n    dates = [2001, 2007, 2012, 1998, 2002, 2003, 2010, 1985, 1997]\r\n    in_text = ['(Guns, ' + str(dates[i]) + ')' for i in range(len(dates))]\r\n\r\n    return in_text\r\n\r\ndef guns_sentence():\r\n    g = 'guns'\r\n    word_count = random.randrange(7,13)\r\n\r\n    word_list = []\r\n    for i in range(word_count):\r\n        word_list.append(g)\r\n\r\n    word_list.append('guns.')\r\n    word_list.insert(0, 'Guns')\r\n\r\n    return ' '.join(map(str, word_list))\r\n\r\ndef guns_paragraph():\r\n    sentence_count = random.randrange(7,20)\r\n\r\n    sentence_list = []\r\n    for i in range(sentence_count):\r\n        sentence_list.append(guns_sentence())\r\n\r\n    in_text = guns_intext()\r\n    s = sentence_list[random.randrange(len(sentence_list))].split()\r\n    s.insert(len(s)/2, in_text[random.randrange(len(in_text))])\r\n\r\n    t = ' '.join(map(str, s))\r\n    sentence_list.insert(random.randrange(len(sentence_list)), t)\r\n\r\n    return ' '.join(map(str, sentence_list))\r\n\r\ndef guns_section():\r\n    paragraph_count = random.randrange(7,15)\r\n\r\n    paragraph_list = []\r\n    for i in range(paragraph_count):\r\n        paragraph_list.append(guns_paragraph())\r\n\r\n    return paragraph_list\r\n\r\ndef guns_headings():\r\n    r = ['I.', 'II.', 'III.', 'IV.', 'V.', 'VI.']\r\n\r\n    headings = [r[i] + ' Guns' for i in range(len(r))]\r\n    return headings\r\n\r\ndef guns_subheadings():\r\n    r = range(4)[1:]\r\n    subheadings = [str(r[i]) + '. ' + guns_paragraph()  for i in range(len(r))]\r\n    return subheadings\r\n\r\ndocument = Document()\r\ndocument.add_heading('Guns', 0)\r\n\r\ngh = guns_headings()\r\n\r\nfor i in range(len(gh)):\r\n    if i == 1:\r\n        document.add_heading(gh[i])\r\n        gsh = guns_subheadings()\r\n        gs1 = guns_section()\r\n        for item in gsh:\r\n            document.add_paragraph(item)\r\n        for x in gs1:\r\n            document.add_paragraph(x)\r\n    elif i == 3:\r\n        document.add_heading(gh[i])\r\n        gsh = guns_subheadings()\r\n        gs1 = guns_section()\r\n        for item in gsh:\r\n            document.add_paragraph(item)\r\n        for x in gs1:\r\n            document.add_paragraph(x)\r\n    elif i == 4:\r\n        document.add_heading(gh[i])\r\n        document.add_paragraph(guns_paragraph())\r\n        recordset = [1.1, 1.2, 1.3, 2.1, 2.2, 2.3, 3.1, 3.2, 3.3]\r\n        table = document.add_table(rows=1, cols=4)\r\n        hdr_cells = table.rows[0].cells\r\n        hdr_cells[0].text = 'Guns and Sub-machine guns'\r\n        hdr_cells[1].text = 'Year 1'\r\n        hdr_cells[2].text = 'Year 2'\r\n        hdr_cells[3].text = 'Year 3'\r\n        for i in range(len(recordset)):\r\n            row_cells = table.add_row().cells\r\n            row_cells[0].text = str(recordset[i])\r\n            row_cells[1].text = 'Guns'\r\n            row_cells[2].text = 'Guns'\r\n            row_cells[3].text = 'Guns'\r\n    else:\r\n        document.add_heading(gh[i])\r\n        gs2 = guns_section()\r\n        for g in gs2:\r\n            document.add_paragraph(g)\r\n\r\ndocument.add_heading('References Cited')\r\ncitations = guns_references()\r\nfor citation in citations:\r\n    document.add_paragraph(citation)\r\n\r\ndocument.save('republican_nsf_proposal.docx')</pre>\n<p>Running this code in your favorite Python environment or command line will output an editable Word doc such as the following:</p>\n<p><a href=\"http://ryan-omizo.com/wp-content/uploads/2015/01/republican_nsf_proposal.docx\">republican_nsf_proposal</a></p>\n<p class=\"gde-text\"><a href=\"http://ryan-omizo.com/wp-content/uploads/2015/01/republican_nsf_proposal.pdf\" class=\"gde-link\" onclick=\"_gaq.push(['_trackEvent', 'Google Doc Embedder', 'Download', this.href]);\">Download (PDF, Unknown)</a></p>\n<iframe src=\"//docs.google.com/viewer?url=http%3A%2F%2Fryan-omizo.com%2Fwp-content%2Fuploads%2F2015%2F01%2Frepublican_nsf_proposal.pdf&amp;hl=en_US&amp;embedded=true\" class=\"gde-frame\" style=\"width:100%; height:500px; border: none;\" scrolling=\"no\"></iframe>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2015%2F01%2F16%2Fpython-recipe-for-nsf-proposals%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<p>In <a href=\"http://ryan-omizo.com/2014/09/18/text-mining-the-mla-job-information-list-part-1/\">Text Mining the MLA Job Information List Part 1</a>, I cobbled together a series of regular expression scripts and list comprehensions in Python to reduce the dimensionality of the October 2012 edition of the MLA Job Information List. This dimensionality reduction removed components such as punctuation, function words, email addresses, and URLs from the source text; it also involved correcting basic but routine OCR scanning errors. The end result was a corpus half the size as the original, non-processed text in terms of token count.</p>\n<p>In this post, I will convert the list of tokens returned by the text processing into bigrams and measure the significance these associations to find word collocations.</p>\n<p>When I refer to <em>bigrams</em>, I am describing tuples of two consecutive tokens. For example, the line, \u201cthis is a test sentence\u201d would be converted to <code>[('this', 'is'), ('is', 'a'), ('a', 'test'), ('test', 'sentence')]</code> if we were extracting bigrams.</p>\n<p>When I refer to <em>collocations</em>, I am describing those bigrams (or any ngram larger than a unigram) that combine to form unitary meanings due to grammar or convention. The most straightforward bigram collocation is a name that relies on a compound such as \u201cNew York\u201d or \u201cEast Lansing.\u201d</p>\n<p>However, collocations also join other linguistic units such as verbs and nouns because convention dictates a constrained usage. For example, in the sentence, \u201cShe opened the gate,\u201d we have the following bigrams (allowing for the removal of \u201cthe\u201d): (\u2018she\u2019, \u2018opened\u2019) and (\u2018opened\u2019, \u2018gate\u2019). These tokens all work to create meaning in this sentence, as is the case with any sentence. However, the example sentence is also amenable to substitution. The tokens \u2018she\u2019 and \u2018opened\u2019 have a subject-verb relationship, but the \u2018opened\u2019 does not strongly depend on \u2018she\u2019 to create meaning; \u2018she\u2019 could just as well be \u2018jane\u2019 or \u2018he\u2019 or \u2018john.\u2019 The case of \u2018opened\u2019 and \u2018gate\u2019 is more complex because conventions in English suggest that we <em>open</em> objects such as doors and gates. It is far less conventional to write, \u201cshe released the gate\u201d or \u201cshe unstopped the gate.\u201d At the same time, \u201cshe unlatched the gate\u201d or \u201cshe unlocked the gate\u201d might also work, suggesting that there is still flexibility between a verb that modifies the object \u201cgate.\u201d A bigram collocation would feature more rigid association between tokens. We might cite \u201cSt. Peter\u2019s Gate\u201d or the \u201cpearly gates.\u201d Both refers to a specific gate, and the replacement of either token would radically change the meaning of the term.</p>\n<p>Tracking and measuring collocations in a natural language text is a common practice and can be applied to numerous information retrieval tasks and research questions. For example, finding stable collocations among tokens can identify compound terms such as \u201creal estate\u201d or \u201cEast Lansing\u201d or \u201cRhode Island.\u201d If such collocations occur at levels of significance greater than random, then a text mining routine can programmatically combine these words into a single token, thereby providing a more accurate representation of a text.</p>\n<p>Given the MLA Job Information List, measuring bigram collocations might contribute to a cleaner dataset. For example, finding significant bigrams might help an analyst differentiate between calls for \u201crhetoric\u201d and \u201crhetoric [and] composition.\u201d At a more basic level, finding significant bigram collocations might help us screen expected compounds such as \u201cenglish department.\u201d This is not a trivial problem, and does merit consideration, especially if we can tune the program to recognize named entities.</p>\n<p>My interest in collocations for these postings is broader, however, or, I should say, less granular. At a basic level, the relevance of ngram collocations is that the proximal relationships between words signify conceptual relationships, and these conceptual relationships influence the structure and meaning of a text. The idea behind this text mining endeavor is to computationally reveal probative insights into the rhetoric of the MLA Job Information List.</p>\n<p>On the face of it, finding significant bigram collocations would only seem to highlight what we already know about the MLA Job Information List. We know that we will see numerous instances of \u201crhetoric, composition\u201d and \u201cenglish, department\u201d or \u201ctechnical, communication.\u201d We would also expect to see collocations between verbs like \u201cmail\u201d and nouns like \u201ccv\u201d or \u201csent\u201d and \u201capplication.\u201d Such collocations all fit the genre of job advertisements in the field of English, rhetoric and composition, professional writing, and creating writing, which provide instructions for applicants and information about delivery of materials.</p>\n<p>If we could imagine this computational experiment undertaken on a blind sample, extracting identifying information might be valuable for the purposes of classification. My primary interest in these posts, however, is to reveal patterns of rhetoric that might not be immediately obvious to conventional readings and to understand how the MLA Job Information List functions as a body of discourse, not just a collection of ads submitted by various institutions.</p>\n<h3>Materials Used</h3>\n<ul>\n<li>Python 2.6+</li>\n<li>NLTK==2.0.4</li>\n<li>Numpy==1.8.0</li>\n</ul>\n<h3>Bigram and Trigram Collocations</h3>\n<p>Creating bigram and trigram collocations is a common practice in natural language processing and several Python libraries already have build-in modules to handle this task, including <a href=\"http://nltk.org\">NTLK</a>. Given that we already have a tokenized list of the October 2012 edition of the MLA Job Information List, we can write our own, brief functions that will turn that list into a list of bigrams and trigrams:</p>\n<pre class=\"height-set:true width-set:true wrap:true lang:default decode:true \">def bigram_finder(tokens):\r\n    return [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\r\n\r\ndef trigram_finder(tokens):\r\n    return [(tokens[i], tokens[i+1], tokens[i+2) for i in range(len(tokens)-2)]\r\n\r\n###If opting to use NLTK\r\n#from nltk import bigrams, trigrams\r\n#bigrams(tokens)\r\n#trigrams(tokens)\r\n</pre>\n<p>The results of the <code>bigram_finder</code> and <code>trigram_finder</code> functions are comparable to the methods found in the NLTK library.</p>\n<h3>Hypothesis Testing</h3>\n<p>While we now have a method to extract collocated tokens from the MLA Job Information List, we have not arrived at a means to test whether or not the collocations are significant. A basic measure of significance is whether or not a particular collocation occurs at a rate greater than random.</p>\n<p>Of course, because we are dealing with a natural language text governed by rules and conventions, not word/token is a product of chance. Consequently, what we are really trying to determine when is if the patterns of collocations in the MLA Job Information List are strongly or weakly militated. In some cases, the associations between tokens are linguistic; however, other associations can point to concept formation and deployment, which can intimate how the rhetoric of the MLA Job Information List is operating computationally.</p>\n<p>To test the strength of the ngram associations, I will use the Student T-Test for significance as outlined by Manning and Schutze (2000 p. 163-166).</p>\n<p>The Student T-Test for significance begins with the null hypothesis that the terms constituting a collocation are independent. In this case, independence means that the likelihood that one term would be collocated with another is no better than chance, which depends on the distribution of the term in the population of terms.</p>\n<p>In order to compare the likelihood that a collocations results from random selection or a more decisive cause, we calculate the <code>t-value</code>. If this <code>t-value</code> is less than a <em>critical value</em> given by a <em>t-table</em>, then we cannot reject the null hypothesis that the collocation exists more or less as a product of chance. If the <code>t-value</code> is greater than the critical value, then we can reject the null hypothesis.</p>\n<p>For bigrams, the <code>t-value</code> is calculated thus:</p>\n<pre class=\"lang:default decode:true \" title=\"Student T-Test for Bigrams\">t_value = (sample_likelihood - independence_likelihood)/(math.sqrt(sample_likelihood/population))\r\n</pre>\n<p>Let\u2019s step through the variables:</p>\n<p>The independence_likelihood is the likelihood that the two tokens in the bigram are collocated at a frequency no better than random. We calculate the independence likelihood of a bigram collocation thus:\n</p><pre class=\"lang:default decode:true \" title=\"Bigram Independence Likelihood\">#Let n_1 be the first token in the bigram\r\n#Let n_2 be the second token in the bigram\r\n#Let the population be the total number of bigrams in the sample\r\n\r\nindependence_likelihood = frequency of n_1/population * frequency of n_2/population </pre>\n<p>In other words, the independence likelihood is the probability that n_1 and n_2 can occur in a sample given their relative frequency; it is the frequency that we would <em>expect</em> to see if chance were the only regulating factor.</p>\n<p>The sample_likelihood is the actual distribution of the bigram in the sample:</p>\n<pre class=\"lang:default decode:true \" title=\"sample likelihood of bigram\">#Let (n_1, n_2) represent the actual bigram\r\n\r\nsample_likelihood = freqency of (n_1, n_2)/population</pre>\n<p>Once again, these variables are assembled in the following equation:</p>\n<pre class=\"lang:default decode:true \" title=\"Student T-Test for Bigrams\">t_value = (sample_likelihood - independence_likelihood)/(math.sqrt(sample_likelihood/population))\r\n</pre>\n<p>NLTK has prebuilt functions to calculate t-values for the Student T-test. However, solving for t-values is not terribly taxing and can be accomplished through the use of Python\u2019s <code>Counter</code> and <code>defaultdict</code>.</p>\n<p>Let\u2019s first dispense with the necessary imports:</p>\n<pre class=\"lang:default decode:true \">from __future__ import division\r\nimport math\r\nfrom collections import Counter, defaultdict</pre>\n<p>Note: You must place <code>from __future__ import division</code> first for it to work.</p>\n<pre class=\"height-set:true width-set:true wrap:true lang:default decode:true \" title=\"Student T-test for Bigrams\">def bigram_student_t(tokenlist):\r\n\r\n    population = len(tokenlist)-1\r\n    counts = Counter(tokenlist)\r\n    bigrams = bigram_finder(tokenlist)\r\n\r\n\r\n    independence_likelihood = defaultdict(list)\r\n    for bigram in bigrams:\r\n        independence_likelihood[bigram] = counts[bigram[0]]/population * counts[bigram[1]]/population\r\n\r\n    sample_likelihood = Counter(bigrams)\r\n    for k, v in sample_likelihood.items():\r\n        sample_likelihood[k] = v/population\r\n\r\n    tvalues = defaultdict(list)\r\n\r\n    for bigram in bigrams:\r\n        tvalues[bigram] = (sample_likelihood[bigram] - independence_likelihood[bigram])/(math.sqrt(sample_likelihood[bigram]/population))\r\n\r\n    return tvalues\r\n</pre>\n<p>Let me step through the code:</p>\n<pre class=\"lang:default decode:true \">population = len(tokenlist)-1\r\ncounts = Counter(tokenlist)\r\nbigrams = bigram_finder(tokenlist)</pre>\n<p>The above three lines of code sets our working variables. Our <code>population</code> refers to the count of bigrams, which will always be 1 less than the length of the input list of tokens.</p>\n<p>The <code>counts</code> variable returns a <code>Counter</code> object, which behaves like a Python dictionary. They <code>keys</code> in <code>counts</code> are the tokens. The <code>values</code> of <code>counts</code> are there frequencies.</p>\n<p><code>bigrams</code> calls are former <code>bigram_finder()</code> function and converts the list of tokens into a list of bigrams.</p>\n<pre class=\"height-set:true width-set:true wrap:true lang:default decode:true \">independence_likelihood = defaultdict(list)\r\n    for bigram in bigrams:\r\n        independence_likelihood[bigram] = counts[bigram[0]]/population * counts[bigram[1]]/population\r\n\r\n    sample_likelihood = Counter(bigrams)\r\n    for k, v in sample_likelihood.items():\r\n        sample_likelihood[k] = v/population\r\n\r\n    tvalues = defaultdict(list)\r\n\r\n    for bigram in bigrams:\r\n        tvalues[bigram] = (sample_likelihood[bigram] - independence_likelihood[bigram])/(math.sqrt(sample_likelihood[bigram]/population)), sample_likelihood * population</pre>\n<p>The remaining code arranges are count information into two <code>defaultdicts</code>. The <code>sample_likelihood</code> <code>defaultdict</code> stores the probability distributions of each bigram in the list of bigrams.</p>\n<p>The <code>independence_likelihood</code> variable stores the probability values of each bigram based on the expected distribution of the bigrams given the independence of each item in the bigram.</p>\n<p>The <code>t_values</code> <code>defaultdict</code> holds our t-value solutions and includes the original bigram count.</p>\n<p>You can obtain similar results by calling NLTK\u2019s <code>BigramAssocMeasures()</code> and <code>BigramCollocationFinder</code></p>\n<pre class=\"height-set:true width-set:true wrap:true lang:default decode:true \" title=\"NLTK Bigram Collocation Finder with Student T-test\">from nltk.collocations import *\r\n\r\nbigram_measures = nltk.collocations.BigramAssocMeasures()\r\n\r\n#Create list of bigrams from tokens\r\nfinder = BigramCollocationFinder.from_words(tokens)\r\n\r\n#Find t values of all bigrams in the list using student t test from Manning and Schutze\r\nfinder.score_ngrams(bigram_measures.student_t)</pre>\n<p>The results from the <code>bigram_student_t</code> function and NLTK\u2019s <code>BigramAssocMeasures()</code> are comparable but not exact. The difference lies in how NLTK defines its population variable. NTLK takes the length of the token list as its population, whereas I have taken the length of the list of bigrams. For example:</p>\n<pre class=\"height-set:true width-set:true wrap:true lang:default decode:true \">#bigram_student_t t-value for ('english', 'department')\r\n#6.98412279000951\r\n\r\n#NLTK BigramAssocMeasures t-value for 'english', 'department'\r\n#6.98414953692</pre>\n<p>The difference is negligible when it comes to checking t-values against a t-table; however, I believe my implementation is more in-keeping with what Manning and Schutze describe.</p>\n<p>Using a <a href=\"http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf\">t-table of critical values</a>, we see that the critical value to reject the null hypothesis of independence for a sample size of 49,444 is 3.291 for a one-tailed test with a 0.9995 degree of confidence. Thus, all those bigrams with a t-value greater than 3.291 are reasonably expected to form collocations.</p>\n<p>You can download a list of the bigrams, their t-values, and their counts <a href=\"http://ryan-omizo.com/wp-content/uploads/2014/09/MLAJIL-10-12.csv.zip\">here as a zipped .csv</a>.</p>\n<p>One thing to note about the t-values: as Manning and Schutze point out, the Student T-test for independence should be considered a means to <em>rank</em> collocations in a text, not to simply declare a word pair a collocation or not. Consequently, you may find bigrams that are normally regarded as collocation lacking a t-value that rises above the critical value.</p>\n<h3>Analysis</h3>\n<p>A perusal of the <a href=\"http://ryan-omizo.com/wp-content/uploads/2014/09/MLAJIL-10-12.csv.zip\">bigram list and their associated t-values and counts</a> may seem a little underwhelming because the output hews so closely to expectation in terms of rhetoric and statistical analysis.</p>\n<p>As is often the case when you tally word or word collocations, the counts form a heavy-tailed distribution. In this case, there are a few collocations that appear at high frequency. But most of the collocations appear once (hence, the heavy tail along the x-axis):</p>\n<figure>\n<a href=\"http://i2.wp.com/ryan-omizo.com/wp-content/uploads/2014/09/figure_1.png\"><img src=\"http://i2.wp.com/ryan-omizo.com/wp-content/uploads/2014/09/figure_1.png?resize=800%2C600\" alt=\"Bigram Distributions\" class=\"alignnone size-full wp-image-462\" srcset=\"http://i2.wp.com/ryan-omizo.com/wp-content/uploads/2014/09/figure_1.png?w=800 800w, http://i2.wp.com/ryan-omizo.com/wp-content/uploads/2014/09/figure_1.png?resize=300%2C225 300w\" sizes=\"(max-width: 800px) 100vw, 800px\" data-recalc-dims=\"1\"></a><br>\n</figure>\n<p>Unsurprisingly, the top-15 bigram collocations in terms of counts comprise the following:</p>\n<ol>\n<li>(\u2018assistant\u2019, \u2018professor\u2019)</li>\n<li>(\u2018apply\u2019, \u2018position\u2019)</li>\n<li>(\u2018department\u2019, \u2018english\u2019)</li>\n<li>(\u2018job\u2019, \u2018information\u2019)</li>\n<li>(\u2018information\u2019, \u2018list\u2019)</li>\n<li>(\u2018mla\u2019, \u2018job\u2019)</li>\n<li>(\u2018english\u2019, \u2018edition\u2019)</li>\n<li>(\u2018list\u2019, \u2018english\u2019)</li>\n<li>(\u2018letter\u2019, \u2018application\u2019)</li>\n<li>(\u2018invite\u2019, \u2018application\u2019)</li>\n<li>(\u2018candidate\u2019, \u2018will\u2019)</li>\n<li>(\u2018writing\u2019, \u2018sample\u2019)</li>\n<li>(\u2018creative\u2019, \u2018writing\u2019)</li>\n<li>(\u2018three\u2019, \u2018letter\u2019)</li>\n</ol>\n<p>For those of us in the field of English studies, rhetoric and composition, professional writing, and creative writing, we can easily interpolate the sentences these bigrams inform. And we are not back to the problem that I posed in the introduction to this post: what can an analysis of bigram collocations tells us about the MLA Job Information List that we don\u2019t already know?</p>\n<p>The answer, I think, is that it may not tell us a lot about the MLA Job Information List, but it can point us to ways in which we can use basic statistical information to track and tag larger units of discourse and to better understand how global meanings arise from more granular elements.</p>\n<p>The above list of bigrams suggests what people in the field of English studies or rhetoric and composition might call boilerplate. Most of the bigrams such as <code>('letter', 'application')</code> and <code>('writing', 'sample')</code> are supplied so that candidates can fulfill the basic requirements of the application process. Through tradition, and legal and institutional norms, the process is relatively homogenous. Thus, the call for applicants looks the same throughout. Call it institutional boilerplate.</p>\n<p>If the top bigrams indicate boilerplating, then they are also indicating a particular rhetorical move aimed at fulfilling genre conventions. If we can use the top bigram collocations to tag segments of boilerplate language using nothing but text normalization and the Student T-test for significance, then, I think, we have found utility for this experiment.</p>\n<p>To these ends, I have written a script that will examine each sentence of the MLA Job Information list, test for the presence of the top-20 collocations, and tag that sentence in HTML with the <code>mark</code> tag.</p>\n<p>You can download the <a href=\"http://ryan-omizo.com/wp-content/uploads/2014/09/bigram_out.html.zip\">tagged HTML file here</a>.</p>\n<h3>Further Questions</h3>\n<p>Whether or not the use of significant bigram collocations can alert us to boilerplate material in the MLA October 2012 Job Information List is up for debate, but I think the results are tantalizing if not definitive.</p>\n<p>Firstly, we see in the frequency distribution chart of the bigrams that the top-20 collocations only comprise a small part of the corpus. However, the deployment of these top-20 collocations has led to almost every sentence of the list being tagged (although there are processing errors in tokenizing sentences). There is no doubt that this is a blunt metric that can/should be refined; but, the results suggests that the generic markers of text can be minute in terms of the overall count of features, but can exert a pervasive effect on the delivery of meaning, which makes intuitive sense if accepted theories of discourse on genre hold.</p>\n<p>These results also call to mind Ridolfo and DeVoss\u2019s article on rhetorical velocity: <a href=\"http://kairos.technorhetoric.net/13.2/topoi/ridolfo_devoss/intro.html\">\u201cCOMPOSING FOR RECOMPOSITION: RHETORICAL VELOCITY AND DELIVERY\u201d</a>. In this piece, Ridolfo and DeVoss examine how writers and designers strategically compose pieces for re-use by other authors and for speedy circulation. One example strategy is boilerplate writing. If the we can tag texts for their use of boilerplate (as defined by a particular context of use), then might we also be able to mathematically gauge the rhetorical velocity of a text\u2013at least in relation to existing forms?</p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2014%2F09%2F25%2Ftext-mining-the-mla-job-information-list-part-2%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<p class=\"gde-text\"><a href=\"http://ryan-omizo.com/wp-content/uploads/2014/03/ROATTW2014.pptx\" class=\"gde-link\" onclick=\"_gaq.push(['_trackEvent', 'Google Doc Embedder', 'Download', this.href]);\">Download (PPTX, Unknown)</a></p>\n<iframe src=\"//docs.google.com/viewer?url=http%3A%2F%2Fryan-omizo.com%2Fwp-content%2Fuploads%2F2014%2F03%2FROATTW2014.pptx&amp;hl=en_US&amp;embedded=true\" class=\"gde-frame\" style=\"width:100%; height:500px; border: none;\" scrolling=\"no\"></iframe>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2014%2F03%2F20%2Fattw-2014-presentation%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<h4>Overview</h4>\n<p>This post is a follow-up to my previous post on hedging, computational rhetoric, and the Wells Report, which details the results of a special investigation into the Jonathan Martin bullying scandal that has recently been the subject of media scrutiny. In that post, I attempted to analyze the Wells Report with a personally developed app called the Hedge-O-Matic. The Hedge-O-Matic uses Naive Bayes classification routines to tag sentences for their hedgey or not-so-hedgey rhetorical content.The Hedge-O-Matic is trained on sentences culled from 150+ academic, science articles. When tested on like genres, the Hedge-O-Matic generally proves 78-82% accurate using a 10-fold cross validation process (90% of the training set is used to determine the hedge or non-hedge quality of a randomly generated 10% of test data). In this post, I weigh the Hedge-O-Matic\u2019s results against a hand-coded version of the Wells Report.</p>\n<h4>Methods</h4>\n<ul>\n<li>After exported the Hedge-O-Matic results to a csv file, I created a separate column for my hand-coded input. Like the Hedge-O-Matic, I coded each sentence as a hedge or non_hedge.</li>\n<li>I then imported this csv in to the data processing library Pandas.</li>\n<li>I then compared the similarity between the Hedge-O-Matic\u2019s tags and my own. If the fields were the same, the row would be tagged as \u201cTrue;\u201d otherwise, \u201cFalse\u201d</li>\n<li>I then calculated accuracy, precision, and recall scores with the \u201cTrue\u201d/\u201dFalse\u201d totals</li>\n</ul>\n<h4>Results and Discussion</h4>\n<h6>Hedge-O-Matic Accuracy = 0.580289456</h6>\n<p>This is a far cry from the 78-82% accuracy that I am accustomed to seeing from the Hedge-O-Matic. In a sense, the app is doing little more than spitballing at the sentences.</p>\n<p>This result is also to be expected given the nature of the training and test sets. The Hedge-O-Matic is tuned for academic, science articles. While formal, the Wells Report is written for a different audience and incorporates different conventions. Moreover, the Wells Report features numerous quotations of text messages, which is another genre by itself. At the present state of development, the Hedge-O-Matic has not been shown anything that resembles a text message, especially not the expletive-laden communications at the heart of the Martin harassment case.</p>\n<p>I will also remind people of a problem that I discussed in the previous post: quotation boundaries. My study is relying on a classification output in which certain sentences escaped tokenization because punctuation was within a quotation. This is a remnant of test done on a literary text, in which many instances of quotations did not end the sentence. As a result, many of the tagged sentences were not single sentences, and this could have shifted the results. When adjusted for the tokenization error, the output appears thus:</p>\n<h6>Hedge-O-Matic Original Length: 1,451 sentences</h6>\n<h6>Hedge-O-Matic Adjusted Length: 1,564 sentences</h6>\n<p>At this time, I have not run the adjusted sentences; however, with a 7.7% loss of sentences, we can expect some degradation of accuracy.</p>\n<p>To provide more clarity on these accuracy figures, here are the tagged distributions of hedge/non-hedge sentences and the correctness of their predictions:</p>\n<table>\n<thead>\n<th>Tag</th>\n<th>#</th>\n<th># Correct</th>\n</thead>\n<tbody>\n<tr>\n<td>Hedge</td>\n<td>708</td>\n<td>132</td>\n</tr>\n<tr>\n<td>Non-Hedge</td>\n<td>743</td>\n<td>710</td>\n</tr>\n</tbody>\n</table>\n<p>These numbers translate to the following precision, recall scores, and F1 scores:</p>\n<table>\n<thead>\n<th>Tag</th>\n<th>Precision</th>\n<th>Recall</th>\n<th>F1 Score</th>\n</thead>\n<tbody>\n<tr>\n<td>Hedge</td>\n<td>0.186440678</td>\n<td>0.8</td>\n<td>0.3024054983263778</td>\n</tr>\n<tr>\n<td>Non-Hedge</td>\n<td>0.955585464</td>\n<td>.55209099533</td>\n<td>0.6998452840307471</td>\n</tr>\n</tbody>\n</table>\n<p>I can attribute the low precision of hedge finding in the Wells Report to a number of factors:</p>\n<ul>\n<li>Borderline hedging/non-hedging with confidence scores around 0.5 are classed as hedges. In other words, when in doubt, the Hedge-O-Matic hedges its own bets by declaring a sentence a hedge.</li>\n<li>The hedging moves made in a legalistic document such as the Wells Report are different than those made in a scientific article, the most notable being reported speech. In many instances, the authors of the Wells Report will quote or recapitulate the sentiments of their interview subjects. Thus, while the interview subject may say something hedgey, the authors themselves are not hedging; they are describing with confidence what they have witnessed.</li>\n</ul>\n<p>There are other more global limitations at work here as well. The most notable is that the training set for the Hedge-O-Matic has not been trained on enough linguistic variability to account for the rhetorical moves made in the Wells Report.</p>\n<p>There is also a high degree of overfitting occurring because of the smallishness and regularity of the training set. Thus, words that are often markers of hedging in scientific articles (\u201chowever,\u201d \u201ccan,\u201d \u201cbelieved,\u201d) are biasing the classifier and predicting hedge sentences even though such words may be the bracketed within an instance of reported speech or paraphrase.</p>\n<p>That said, I would contend that the low accuracy of the Hedge-O-Matic in the case of the Wells Report is actually a good result because it supports the pivotal assumption girding this computational rhetorics project\u2013that different discourses feature different conventions that signal disciplinary and generic boundaries and that these boundaries can be traced by computers.</p>\n<h6>Part 3 of this study will focus on visualizing the results.</h6>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2014%2F02%2F24%2Fhedging-and-the-jonathan-martin-bullying-scandal-part-2%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<p>On October 26, 2013, I was invited to speak at the Midwestern Conference of Asian Affairs on digital humanities and the work being done by <a href=\"http://matrix.msu.edu\">MATRIX</a> and <a href=\"www2.matrix.msu.edu/wide/\">WIDE</a>. I described our methods of using graph and social network analytical metrics to extract rhetorical moves from discourse. I even got a chance to test our theories on a dataset that I used in my dissertation involving around 36,000 YouTube comments on a vide by <a href=\"http://www.youtube.com/user/MRirian\">MriRian</a>.</p>\n<p>I hope to discuss my findings of this experiment in more detail in a future post. Here, I wanted to express my gratitude for Professor Ethan Segal and <a href=\"http://matrix.msu.edu\">MATRIX</a> for allowing me to present on out work in WIDE-MATRIX\u2019s Computational Rhetoric group, and to thank the assembled audience of Asian Studies scholars. The group asked keen questions about the unruly, noisy nature of texts, which always reminds me the rhetorical acts implicit in the normalization of data. The act of cleaning data is always a lossy activity, those who do text or data mining must not only be cognizant of the the data that we are screening out but also be as transparent as possible about what our biases and rationale are for these protocols. Methods carry with them methodologies, and these methodologies are inflected by ideological orientations that will condition the data. </p>\n<p>The term heuristic was also used a lot in our discussions, and to me, that is one of the most valuable and focusing glosses for the work that we are doing in the computational rhetorics group. We are creating and applying critical lens to data in order to build theory. But these lens still require human tuning to arrive at plausible interpretations. </p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2013%2F10%2F27%2Fmcaa-presention-on-computational-rhetorics%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<p>The kind folks as MATRIX, MSU\u2019s Center for Humane Arts, Letters, and Social Sciences has featured a recent talk on computational rhetoric I gave with Bill Hart-Davidson at Computers and Writing 2013.</p>\n<p>Thanks to MATRIX for supporting the our initiatives in computational rhetoric.</p>\n<p>See the blog post here: <a href=\"http://bit.ly/1a4FVPC\">http://bit.ly/1a4FVPC</a>.</p>\n\n<div class=\"twitter-share\"><a href=\"https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2013%2F06%2F23%2Fcwcon-presentation-featured-on-matrix-blog-today%2F&amp;via=OmizoRm\" class=\"twitter-share-button\">Tweet</a></div>\n                 \n                    </div>", "<div class=\"entry-content\">\n<p>One of the most fundamental concerns when envisioning methodologies of computational rhetoric is this: can we turn a natural language text\u2013with all its rhetorical intricacies\u2013into a computational object?</p>\n<p>If we can, then this obliges another question, what kind?</p>\n<p>The issue can be framed in terms of data structures, namely, what kind of data structure does a natural language text offer? If a computer is to read it, then that text must offer some form data structure.</p>\n<p>So what is the best fit?</p>\n<p>My first impulse is to say that a graph is the best data structure for a natural language text.</p>\n<p>If this is true, then this theory will a great deal to affirm post-structuralist theories of language as well as challenge existing models.</p>\n                 \n                    </div>"], "start_url": "http://ryan-omizo.com/experiments-blog-page/", "links": ["http://ryan-omizo.com/", "http://ryan-omizo.com/", "http://ryan-omizo.com/cv-page/", "http://ryan-omizo.com/research-page/", "http://ryan-omizo.com/teaching-page/", "http://ryan-omizo.com/experiments-blog-page/", "#content", "https://twitter.com/omizorm", "https://twitter.com/intent/follow?screen_name=OmizoRM", "https://twitter.com/OmizoRM", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2016%2F01%2F11%2Ffinding-genre-signals-in-academic-writing-benchmarking-method%2F&via=OmizoRm", "http://politico.com", "http://www.politico.com/magazine/story/2015/01/no-the-gop-is-not-at-war-with-science-114195.html#.VLiTY3YppbQ", "http://www.politico.com/magazine/story/2015/01/no-the-gop-is-not-at-war-with-science-114195.html#.VLiTY3YppbQ", "http://ryan-omizo.com/wp-content/uploads/2015/01/republican_nsf_proposal.docx", "http://ryan-omizo.com/wp-content/uploads/2015/01/republican_nsf_proposal.pdf", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2015%2F01%2F16%2Fpython-recipe-for-nsf-proposals%2F&via=OmizoRm", "http://ryan-omizo.com/2014/09/18/text-mining-the-mla-job-information-list-part-1/", "http://nltk.org", "http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf", "http://ryan-omizo.com/wp-content/uploads/2014/09/MLAJIL-10-12.csv.zip", "http://ryan-omizo.com/wp-content/uploads/2014/09/MLAJIL-10-12.csv.zip", "http://i2.wp.com/ryan-omizo.com/wp-content/uploads/2014/09/figure_1.png", "http://ryan-omizo.com/wp-content/uploads/2014/09/bigram_out.html.zip", "http://kairos.technorhetoric.net/13.2/topoi/ridolfo_devoss/intro.html", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2014%2F09%2F25%2Ftext-mining-the-mla-job-information-list-part-2%2F&via=OmizoRm", "http://ryan-omizo.com/wp-content/uploads/2014/03/ROATTW2014.pptx", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2014%2F03%2F20%2Fattw-2014-presentation%2F&via=OmizoRm", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2014%2F02%2F24%2Fhedging-and-the-jonathan-martin-bullying-scandal-part-2%2F&via=OmizoRm", "http://matrix.msu.edu", "www2.matrix.msu.edu/wide/", "http://www.youtube.com/user/MRirian", "http://matrix.msu.edu", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2013%2F10%2F27%2Fmcaa-presention-on-computational-rhetorics%2F&via=OmizoRm", "http://bit.ly/1a4FVPC", "https://twitter.com/intent/tweet?url=http%3A%2F%2Fryan-omizo.com%2F2013%2F06%2F23%2Fcwcon-presentation-featured-on-matrix-blog-today%2F&via=OmizoRm"], "title": ["ryan-omizo.com | "]}]